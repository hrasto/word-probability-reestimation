{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting folders from the original zip-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfile = \"D:\\\\en.zip\"\n",
    "years = [str(year) for year in range(2000, 2019)]\n",
    "to_extract = ['OpenSubtitles/xml/en/{}'.format(year) for year in years]\n",
    "\n",
    "with ZipFile(zipfile, 'r') as archive:\n",
    "    for path in to_extract:\n",
    "        archive.extract(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering the original archive\n",
    "\n",
    "Goal is to delete redundant files that don't need to be kept. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'out'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "path_dir = 'OpenSubtitles/xml/en'\n",
    "years = os.listdir(path_dir)\n",
    "movies = {year: os.listdir(os.path.join(path_dir, year)) for year in years}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nto_delete = []\\nto_keep = []\\nyears_skip = []\\n\\nfor year, year_movies in movies.items():\\n    if year in years_skip:\\n        print(\"Skipping {}...\".format(year))\\n        continue\\n    print(\"Year {} ({} movies)\".format(year, len(year_movies)))\\n    for movie in year_movies: \\n        movie_path = os.path.join(path_dir, year, movie)\\n        alternatives = os.listdir(movie_path)\\n        success = False\\n        for i, alt in enumerate(alternatives):\\n            xml_path = os.path.join(movie_path, alt)\\n            if not success:\\n                with open(xml_path, \\'r\\', encoding=\\'utf-8\\') as xml_file:\\n                    try:\\n                        tree = ET.parse(xml_file)\\n                        success = True\\n                        to_keep.append(xml_path)\\n                    except Exception as e:\\n                        to_delete.append(xml_path)\\n            else:\\n                to_delete.append(xml_path)\\n        if not success:\\n            print(\"No xml available for {}\".format(movie_path))\\n#print(to_delete)\\npickle.dump({\\'to_delete\\': to_delete, \\'to_keep\\': to_keep}, open(\\'to_delete_keep.pkl\\', \\'wb\\'))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with checking xml\n",
    "'''\n",
    "to_delete = []\n",
    "to_keep = []\n",
    "years_skip = []\n",
    "\n",
    "for year, year_movies in movies.items():\n",
    "    if year in years_skip:\n",
    "        print(\"Skipping {}...\".format(year))\n",
    "        continue\n",
    "    print(\"Year {} ({} movies)\".format(year, len(year_movies)))\n",
    "    for movie in year_movies: \n",
    "        movie_path = os.path.join(path_dir, year, movie)\n",
    "        alternatives = os.listdir(movie_path)\n",
    "        success = False\n",
    "        for i, alt in enumerate(alternatives):\n",
    "            xml_path = os.path.join(movie_path, alt)\n",
    "            if not success:\n",
    "                with open(xml_path, 'r', encoding='utf-8') as xml_file:\n",
    "                    try:\n",
    "                        tree = ET.parse(xml_file)\n",
    "                        success = True\n",
    "                        to_keep.append(xml_path)\n",
    "                    except Exception as e:\n",
    "                        to_delete.append(xml_path)\n",
    "            else:\n",
    "                to_delete.append(xml_path)\n",
    "        if not success:\n",
    "            print(\"No xml available for {}\".format(movie_path))\n",
    "#print(to_delete)\n",
    "pickle.dump({'to_delete': to_delete, 'to_keep': to_keep}, open('to_delete_keep.pkl', 'wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withOUT checking xml (faster)\n",
    "to_delete = []\n",
    "to_keep = []\n",
    "for year, year_movies in movies.items():\n",
    "    print(\"Year {} ({} movies)\".format(year, len(year_movies)))\n",
    "    for movie in year_movies: \n",
    "        movie_path = os.path.join(path_dir, year, movie)\n",
    "        alternatives = os.listdir(movie_path)\n",
    "        to_keep.append(alternatives[0])\n",
    "        if len(alternatives) > 1:\n",
    "            to_delete += alternatives[1:]\n",
    "            \n",
    "#pickle.dump({'to_delete': to_delete, 'to_keep': to_keep}, open(os.path.join(output_dir, 'to_delete_keep.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(os.path.join(output_dir, 'to_delete_keep.pkl'), 'rb') as f:\n",
    "#    dk1 = pickle.load(f)    \n",
    "#to_delete = dk1['to_delete']\n",
    "#to_keep = dk1['to_keep']\n",
    "len(to_keep), len(to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete redundant files\n",
    "\n",
    "Deletes files previously extracted from the original zip file that don't need to be kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in to_delete: \n",
    "    try:\n",
    "        os.remove(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print('File not found', file_path)\n",
    "    path, filename = os.path.split(file_path)\n",
    "    if os.path.isdir(path) and len(os.listdir(path)) == 0:\n",
    "        try:\n",
    "            os.rmdir(path)\n",
    "            print('Directory {} removed'.format(path))\n",
    "        except FileNotFoundError as e:\n",
    "            print('Directory not found', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and lemmatization\n",
    "\n",
    "This step takes a long time. On my laptop it ran for about a week. \n",
    "It produces a pickle file for every year containing a dictionary indexed by movie IDs and containing word-count dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bprint_every = 1000\n",
    "counter = 0\n",
    "limit = -1\n",
    "num_movies = sum([len(movie_list) for year, movie_list in movies.items()])\n",
    "to_process = (min(limit, num_movies) if limit > 0 else num_movies)\n",
    "print('{} movies to process'.format(to_process))\n",
    "print('{} movies in total'.format(num_movies))\n",
    "\n",
    "time_start = time.time()\n",
    "for year, movie_list in movies.items():\n",
    "    movie_word_counts = {}\n",
    "    print('Year {}'.format(year))\n",
    "    for movie in movie_list:\n",
    "        if counter % print_every == 0 and counter > 0:\n",
    "            rate = print_every/(time.time()-time_start)\n",
    "            eta = (to_process-counter)/rate\n",
    "            print('{}/{}, rate={}/s, eta=t+{}s ({})'.format(counter, num_movies, rate, eta, datetime.fromtimestamp(time.time()+int(eta))))\n",
    "            time_start = time.time()\n",
    "        movie_path = os.path.join(path_dir, year, movie)\n",
    "        xml_path = os.path.join(movie_path, os.listdir(movie_path)[0])\n",
    "        word_counts = {}\n",
    "        with open(xml_path, 'r') as xml_file:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            for child in root:\n",
    "                sequence = ''\n",
    "                for w in child.findall('w'):\n",
    "                    if w.text[0]==\"'\":\n",
    "                        sequence += w.text\n",
    "                    elif len(sequence) == 0:\n",
    "                        sequence += w.text\n",
    "                    else:\n",
    "                        sequence += ' '+w.text\n",
    "                tokens = nlp(sequence)\n",
    "                lemmas = [str(token.lemma_).lower() for token in tokens if not token.is_punct]\n",
    "                for lemma in lemmas:\n",
    "                    word_counts[lemma] = word_counts.get(lemma, 0)+1\n",
    "        movie_word_counts[movie] = word_counts\n",
    "        counter += 1\n",
    "        if counter == limit:\n",
    "            break\n",
    "    pickle.dump(movie_word_counts, open(os.path.join(output_dir, '{}-word-counts.pkl'.format(str(year))), 'wb'))\n",
    "    if counter == limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it together\n",
    "\n",
    "Create a sparse word-document matrix and a vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = os.listdir(path_dir)[:-1] # leave out 2018\n",
    "corpus_by_year = {}\n",
    "total=0\n",
    "\n",
    "for year in years:\n",
    "    word_count = {}\n",
    "    failed_count = 0\n",
    "    with open(os.path.join(output_dir, '{}-word-counts.pkl'.format(year)), 'rb') as f:\n",
    "        movies = pickle.load(f)\n",
    "        print('Year {}, {} movies'.format(year, len(movies)))\n",
    "        for movie, wc in movies.items():\n",
    "            for word, count in wc.items():\n",
    "                word_count[word] = word_count.get(word, 0)+count\n",
    "                total+=count\n",
    "    corpus_by_year[year]=word_count\n",
    "print('{} words in total'.format(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = pd.read_csv('./BLP/blp-items.txt', sep='\\t', usecols=['spelling', 'lexicality'])\n",
    "stimuli = stimuli[stimuli['lexicality']=='W']['spelling'].values.flatten()\n",
    "\n",
    "vocab = set()\n",
    "for year, wc in corpus_by_year.items():\n",
    "    vocab = vocab.union(wc.keys())\n",
    "print(len(vocab))\n",
    "\n",
    "vocab_blp = vocab.intersection(stimuli)\n",
    "vocab_blp = sorted(list(vocab_blp))\n",
    "word2idx = {w:i for i, w in enumerate(vocab_blp)}\n",
    "len(vocab_blp)\n",
    "\n",
    "pickle.dump(vocab_dict, open(os.path.join(output_dir, 'vocab.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_ndocs = []\n",
    "for year in years:\n",
    "    with open(os.path.join(output_dir, '{}-word-counts.pkl'.format(year)), 'rb') as f:\n",
    "        movies = pickle.load(f)\n",
    "        yearly_ndocs.append(len(movies))\n",
    "ndocs = sum(yearly_ndocs)\n",
    "print('{} documents in total'.format(ndocs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
